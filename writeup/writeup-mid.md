# 3D Object detection in LIDAR scans

## Section 1 : Compute Lidar Point-Cloud from Range Image

### Visualize range image channels
LIDAR data includes range measurements and reflected intensity measurements. In this activity, range channel and intensity channels of LIDAR are stacked together in one image as below. Top half is the range image, and bottom half is the intensity channel.

![image](resources/range_image_stacked.png)

### Visualize LIDAR point cloud
Point cloud is generated by projecting the LIDAR range image into a 3D space. Then this point cloud is visualized in following figures as it is iterated over the consecutive frames, and rendered from different viewpoints. 

Point colour indicates the height of the point. For example red/brown indicates highest elavation and dark blue refers to the lowest elavation. As the point cloud is observed from different point of view, we can see different obstructed and visibility of the objects.

point cloud view #1
![image](resources/pcl1.png)
point cloud view #2
![image](resources/pcl2.png)
point cloud view #3
![image](resources/pcl3.png)
point cloud view #4


![image](resources/pcl4.png)

point cloud view #5

![image](resources/pcl5.png)
point cloud view #6
![image](resources/pcl6.png)

point cloud view #7
![image](resources/pcl7.png)
point cloud view #8
![image](resources/pcl8.png)
point cloud view #9
![image](resources/pcl9.png)
point cloud view #10
![image](resources/pcl10.png)
point cloud view #11
![image](resources/pcl11.png)
point cloud view #12
![image](resources/pcl12.png)

As it is observed in the below zoomed range image (top half - range channel, bottom half - intensity channel), it can be clearly observed in the intensity channel, vehicle main body, rear bumpers, tail lights, and head lights have high intensity values (more white). This should be related to high LIDAR reflection (e.g. head and tail lights) and larger continous surface area (e.g. vehicle main body and rear bumpers) of those. With these high responsive nature, main behicle body, bumpers, tail and head lights are good stable features for the detection algorithms.

![image](resources/range_image_zoomed.png)

## Section 2 : Create Birds-Eye View from Lidar PCL
### Convert sensor coordinates to BEV-map coordinates

Bird eye view (BEV) is generated from rendering the observation from the down looked view, referenced aroud the X,Y coordinates in sensor space. 

![image](resources/bev.png)
### Compute intensity layer of the BEV map

After projecting the LIDAR point cloud into the BEV, intenstiy channel BEV is generated by collecting the intensity value of each corresponding topmost point. Intensity values are normalized in the range of 0 to 255.

![image](resources/bev1.png)

### Compute height layer of the BEV map

After projecting the LIDAR point cloud into the BEV, point height BEV is generated by collecting the height value of each corresponding topmost point. Height values are normalized in the range of 0 to 255.

![image](resources/bev2.png)

## Section 3 : Model-based Object Detection in BEV Image
### Add a second model from a GitHub repo
In this exercise, we use FPN_RESNET prior trained ML model to detect vehicles in the provided point cloud.

### Extract 3D bounding boxes from model response

Using the pre trained FPN_RESENT model, now we are going to detect the vehicles and draw a bounding box around the detected vehicles. We will do this in following steps.

Step 1: Input point cloud

![image](resources/obj1.png)

Step 2: Extracted BEV range channel

![image](resources/obj2.png)

Step 3: Extracted BEV intensity channel 

![image](resources/obj3.png)

Step 4: Inference of vehicles using FPN_RESNET pre trained model 

Legend: green bounding box - ground truth, red bounding box - inference detections

![image](resources/obj4.png)

Step 5: Ground truth vs inference detections rendered into the camera and LIDAR intensity BEV views.

Legend: blue bounding box - ground truth, green bounding box - inference detections.

![image](resources/obj5.png)

Another consecutive frames with ground truth and inference detections is shown below.

![image](resources/obj6.png)

## Section 4 : Performance Evaluation for Object Detection

Performance of darknet model to detect vehicles in the LIDAR point cloud is evaluted using the ground truth labels and the algorithm detections.

### Compute intersection-over-union between labels and detections
By varying the intersection-over union (IOU), we can see how closely the inference detected objects overlap with the ground truth. 

### Compute precision and recall
With the pre-trained darknet model we achieve precision of 0.996 and recall of 0.813725.. which is quite good, it is worth to note false negatives could result in potential hazards, specially if those vehicles are in the proximity for a collision. 

![image](resources/objeval4.png)

By setting the flag configs_det.use_labels_as_objects to True, we can force the results to be taken from the ground truth lables, causing the resulting performance to be perfect (precision = 1.0 and recall = 1.0), and indicating the computations working fine, but this is not the actual detections.

![image](resources/objeval5.png)

![image](resources/objeval7.png)

Summary: In this project, we use LIDAR range/intenstiy scans with generated point clouds to detect vehicles in the surrounding. Using the pre-trained Darknet model, system achieved precision of 0.996 and recall of 0.813725. It is important to note, false negatives for the vehicles in the proximity could end up in potential collision. That's where multiple sensors could work together to eliminate or reduce the risk of specially the false negatives.